'''
This file input the summary of gene_name, and output the gene descriptions generated by selected LLM. 
Currently We support GPT-4o and Deepseek_v3, deepseek_reason, llama-2
'''
import sys
import requests
import json
import os
import numpy as np
from openai import OpenAI
from huggingface_hub import login
from huggingface_hub import HfApi, Repository
from transformers import AutoTokenizer, LlamaForCausalLM

def run_gpt_api(st, max_tokens=500):
    """
    request gpt-4o
    """
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key="xxxxxxxxxxxx" #please use your key here
    )

    response = client.chat.completions.create(
        model="openai/gpt-4o",
        messages=[
            {"role": "user", "content": st}
        ],
        max_tokens=max_tokens
    )
    # analyze request result
    try:
        reply = response.choices[0].message.content
        return reply
    except:
        print("error message：", response.text) 
        return None

def run_deepseek_api(st, deepseek_type):
    """
    跑deep seek 的文件
    """
    if deepseek_type == "deepseek_v3":
        deep_model = "deepseek-chat"
    elif deepseek_type == "deepseek_r":
        deep_model = "deepseek-reasoner"
    client = OpenAI(api_key="xxxxxxxxxxxx", base_url="https://api.deepseek.com") #please use your key here
    response = client.chat.completions.create(
        model=deep_model,
        messages=[
            {"role": "system", "content": "You are a biological scientist"},
            {"role": "user", "content": st},
        ],
        stream=False
    )
    return response.choices[0].message.content

def run_llama(st):
    login(token="xxxxxxxxxxxx") #please use your key here
    api = HfApi()
    model_name = "meta-llama/Llama-2-7b-chat-hf"
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf") 
    model = LlamaForCausalLM.from_pretrained(model_name)  
    input_ids = tokenizer.encode(st, return_tensors="pt")
    output = model.generate(input_ids, max_length=1000, num_return_sequences=1)
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    return generated_text

def request_gpt_data(data, LLM_type, gene_group):
    #request LLM for result
    gene_name = data["gene_name"]
    Maps_display_str = ""
    Entrezgene_summary = ""
    if "Entrezgene_location" in data:
        Entrezgene_location = data["Entrezgene_location"]
        if "Maps_display-str" in Entrezgene_location:
            Maps_display_str = Entrezgene_location["Maps_display-str"]
    if "Entrezgene_summary" in data:
        Entrezgene_summary = data["Entrezgene_summary"]
    #build up request data
    gene_cancer_group = {
        "stnet": "breast cancer",
        "skin": "skin cancer",
        "her2st": "breast cancer",
    }
    st = '''
    ###task: 
    You are a biology scientist specialising in gene study. Your mission is to describe the functionality and phenotype of the gene based on the input gene information and reference. 
    The gene gave to you is related to {} in humans.
    ####Constraints:
    1. The output format should be json, including 3 keys ‘gene symbol’, ‘functionality’ and ‘phenotype’. 
    2. The output value should string, it should be concise and contain keywords only.
    3. The sum number of all output value should be less than 90 words.
    4. There shouldn't be adjectives in output value, such as 'crucial' 
    5. You are encouraged to complement the missing information if you cannot find enough information in reference. 
    ###input:
    Gene symbol: {}
    gentrezgene_location: {}
    entrezgene_summary: {}
    ###output: 
    '''.format(gene_cancer_group[gene_group], gene_name, Maps_display_str, Entrezgene_summary)
    #request LLM
    if LLM_type == "gpt-4o":
        LLM_result = run_gpt_api(st)
    elif LLM_type in ["deepseek_v3", "deepseek_r"]:
        LLM_result = run_deepseek_api(st, LLM_type)
    elif LLM_type == "llama2":
        LLM_result = run_llama(st)
    else:
        print("Sorry, we do not support this LLM currently")
        return ""
    result_str = generate_json_str(LLM_result)
    return result_str

def generate_json_str(result):
    """
    analyze the datasets generated by LLMs
    """
    result = result[result.index("{"):]
    last_index = -1
    for i in range(len(result)):
        if result[i] == '}':
            last_index = i
    result = result[:last_index+1]
    result = json.loads(result)
    #combine data into proper format
    result_str = ""
    gene_symbol = result["gene symbol"]
    functionality = result["functionality"]
    phenotype = result["phenotype"]
    result_str += "gene symbol: " + gene_symbol + ". "
    result_str += "Functionality: " + functionality.lower() + " "
    result_str += "Phenotype: " + phenotype.lower() + " "
    return result_str

def generate_main_result(gene_filename, gene_filepath, output_file, LLM_type, gene_group):
    """
    generate LLM description result for input gene_filename
    """
    data = np.load(gene_filename, allow_pickle=True)
    for gene_name in data:
        gene_path = gene_filepath + "/" + gene_name
        try:
            with open(gene_path, "r") as f:
                gene_data = json.load(f)
        except:
            print("no summary for gene: " + gene_name)
            continue
        if 1==1:
            result_str = request_gpt_data(gene_data, LLM_type, gene_group)
        else:
            print("error when generating LLM descriptions")
            continue
        with open(output_file, "a+") as n:
            n.write(gene_name + "\t" + result_str + "\n")

if __name__ == "__main__":
    gene_filename = sys.argv[1]
    gene_filepath = sys.argv[2]
    output_file = sys.argv[3]
    LLM_type = sys.argv[4]
    gene_group = sys.argv[5]
    generate_main_result(gene_filename, gene_filepath, output_file, LLM_type, gene_group)
